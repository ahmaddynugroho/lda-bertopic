{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./datasets/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from utils import tokenize\n",
    "\n",
    "def build_dataset(df, column_name):\n",
    "    result = {}\n",
    "    result['name'] = column_name\n",
    "    result['tokenized_texts'] = tokenize(df[column_name])\n",
    "    result['dictionary'] = Dictionary(result['tokenized_texts'])\n",
    "    result['corpus'] = [result['dictionary'].doc2bow(s) for s in result['tokenized_texts']]\n",
    "    return result\n",
    "\n",
    "def get_topics(lda, dictionary):\n",
    "    k = lda.num_topics\n",
    "    result = []\n",
    "    for i in range(k):\n",
    "        terms = lda.get_topic_terms(i)\n",
    "        ki = [dictionary[t[0]] for t in terms]\n",
    "        result.append(ki)\n",
    "    return { 'topics': result }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_short_raw = build_dataset(df, 'short_raw')\n",
    "dataset_long_raw = build_dataset(df, 'long_raw')\n",
    "dataset_short = build_dataset(df, 'short')\n",
    "dataset_long = build_dataset(df, 'long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "import optuna\n",
    "\n",
    "def create_lda(d):\n",
    "    def lda(trial):\n",
    "        num_topics = trial.suggest_int('num_topics', 5, 100)\n",
    "        alpha = trial.suggest_float('alpha', 0.001, 1)\n",
    "        eta = trial.suggest_float('eta', 0.001, 1)\n",
    "        return LdaMulticore(d['corpus'], num_topics, d['dictionary'], alpha=alpha, eta=eta, random_state=99)\n",
    "    return lda\n",
    "\n",
    "def create_objective(d):\n",
    "    def objective(trial):\n",
    "        lda = create_lda(d)(trial)\n",
    "        tc = Coherence(texts=d['tokenized_texts'], measure='c_v')\n",
    "        tc_score = tc.score(get_topics(lda, d['dictionary']))\n",
    "        return tc_score\n",
    "    return objective\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_trial(dataset):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(create_objective(dataset), n_trials=2)\n",
    "    return study.best_trial\n",
    "\n",
    "def get_best_lda(dataset):\n",
    "    best_trial = get_best_trial(dataset)\n",
    "    lda = create_lda(dataset)(best_trial)\n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "import time\n",
    "\n",
    "def evaluate_lda(dataset):\n",
    "    start = time.time()\n",
    "    lda = get_best_lda(dataset)\n",
    "    topics = get_topics(lda, dataset['dictionary'])\n",
    "    tc = Coherence(dataset['tokenized_texts'], measure='c_v')\n",
    "    td = TopicDiversity()\n",
    "    return {\n",
    "        'dataset': dataset['name'],\n",
    "        'elapse_time': time.time() - start,\n",
    "        'coherence_score': tc.score(topics),\n",
    "        'diversity_score': td.score(topics)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-07 10:06:09,297] A new study created in memory with name: no-name-43347d20-42aa-4b93-be3a-5b54e00d5404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-07 10:06:10,138] Trial 0 finished with value: 0.325855386775041 and parameters: {'num_topics': 78, 'alpha': 0.5546556288289214, 'eta': 0.22830578967829662}. Best is trial 0 with value: 0.325855386775041.\n",
      "[I 2023-08-07 10:06:10,899] Trial 1 finished with value: 0.3275929614296654 and parameters: {'num_topics': 78, 'alpha': 0.5252980289357414, 'eta': 0.20763160928906513}. Best is trial 1 with value: 0.3275929614296654.\n",
      "[I 2023-08-07 10:06:11,668] A new study created in memory with name: no-name-9119dc7d-141e-42f5-8f92-3c65c370c4d9\n",
      "[I 2023-08-07 10:06:15,123] Trial 0 finished with value: 0.3389515593334336 and parameters: {'num_topics': 95, 'alpha': 0.35002645409204075, 'eta': 0.1265329151340699}. Best is trial 0 with value: 0.3389515593334336.\n",
      "[I 2023-08-07 10:06:17,935] Trial 1 finished with value: 0.34221352353166207 and parameters: {'num_topics': 98, 'alpha': 0.9562680424055888, 'eta': 0.7792445781500905}. Best is trial 1 with value: 0.34221352353166207.\n",
      "[I 2023-08-07 10:06:21,066] A new study created in memory with name: no-name-40b044c6-d089-4a28-a334-b57b12774a61\n",
      "[I 2023-08-07 10:06:21,627] Trial 0 finished with value: 0.5455003455179853 and parameters: {'num_topics': 56, 'alpha': 0.5437594402997574, 'eta': 0.06174190109804764}. Best is trial 0 with value: 0.5455003455179853.\n",
      "[I 2023-08-07 10:06:22,740] Trial 1 finished with value: 0.4730692096418699 and parameters: {'num_topics': 52, 'alpha': 0.06926811821497945, 'eta': 0.16587104712042422}. Best is trial 0 with value: 0.5455003455179853.\n",
      "[I 2023-08-07 10:06:23,286] A new study created in memory with name: no-name-cba3b270-6596-45ec-a8e3-6bd106eb7230\n",
      "[I 2023-08-07 10:06:24,197] Trial 0 finished with value: 0.3691184572594868 and parameters: {'num_topics': 41, 'alpha': 0.8001162496788761, 'eta': 0.3679885509876032}. Best is trial 0 with value: 0.3691184572594868.\n",
      "[I 2023-08-07 10:06:26,001] Trial 1 finished with value: 0.3752821959446844 and parameters: {'num_topics': 85, 'alpha': 0.556712531476334, 'eta': 0.9201963835391491}. Best is trial 1 with value: 0.3752821959446844.\n"
     ]
    }
   ],
   "source": [
    "lda_evaluation = [\n",
    "    evaluate_lda(dataset_short_raw),\n",
    "    evaluate_lda(dataset_long_raw),\n",
    "    evaluate_lda(dataset_short),\n",
    "    evaluate_lda(dataset_long),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_evaluation_df = pd.DataFrame(lda_evaluation)\n",
    "# lda_evaluation_df.to_csv('./results/lda_evaluation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>elapse_time</th>\n",
       "      <th>coherence_score</th>\n",
       "      <th>diversity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>short_raw</td>\n",
       "      <td>1.724713</td>\n",
       "      <td>0.327593</td>\n",
       "      <td>0.023077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>long_raw</td>\n",
       "      <td>8.320421</td>\n",
       "      <td>0.342214</td>\n",
       "      <td>0.011224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>short</td>\n",
       "      <td>1.785719</td>\n",
       "      <td>0.545500</td>\n",
       "      <td>0.055357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>long</td>\n",
       "      <td>4.044739</td>\n",
       "      <td>0.375282</td>\n",
       "      <td>0.022353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset  elapse_time  coherence_score  diversity_score\n",
       "0  short_raw     1.724713         0.327593         0.023077\n",
       "1   long_raw     8.320421         0.342214         0.011224\n",
       "2      short     1.785719         0.545500         0.055357\n",
       "3       long     4.044739         0.375282         0.022353"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_evaluation_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
