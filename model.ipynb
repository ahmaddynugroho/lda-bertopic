{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_FILE_PREPROCESSED = './results/ds.parquet'\n",
    "PATH_FILE_ELAPSE_TIME = './results/elapse_time.csv'\n",
    "PATH_PREFIX_MODEL_LDA = './results/models_lda/'\n",
    "PATH_PREFIX_MODEL_BERTOPIC = './results/models_bertopic/'\n",
    "LDA_N_TRIAL = 1 # FIXME: should be 100 fr fr\n",
    "USE_GPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds = pd.read_parquet(PATH_FILE_PREPROCESSED)\n",
    "elapse_time = pd.read_csv(PATH_FILE_ELAPSE_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/civbag/mambaforge/envs/lba/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/civbag/mambaforge/envs/lba/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/civbag/mambaforge/envs/lba/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/civbag/mambaforge/envs/lba/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from utils import get_diversity, get_topics_lda\n",
    "import optuna\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def lda(docs):\n",
    "    docs = docs.dropna()\n",
    "    dictionary = Dictionary(docs)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    def objective(trial, get_lda=False):\n",
    "        num_topics = trial.suggest_int('num_topics', 5, 100)\n",
    "        alpha = trial.suggest_categorical('alpha_categorical', ['symmetric', 'asymmetric', 'scalar'])\n",
    "        eta = trial.suggest_categorical('eta_categorical', ['symmetric', 'auto', 'scalar'])\n",
    "        if alpha == 'scalar':\n",
    "            alpha = trial.suggest_float('alpha', 0.01, 1)\n",
    "        if eta == 'scalar':\n",
    "            eta = trial.suggest_float('eta', 0.01, 1)\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num_topics,\n",
    "            alpha=alpha,\n",
    "            eta=eta,\n",
    "        )\n",
    "        if get_lda:\n",
    "            return model\n",
    "        c = CoherenceModel(model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "        cs = c.get_coherence()\n",
    "        ts = get_diversity(get_topics_lda(model, dictionary))\n",
    "        return cs * ts\n",
    "\n",
    "    t_start = time()\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=LDA_N_TRIAL)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_lda = objective(best_trial, get_lda=True)\n",
    "    t_end = time()\n",
    "\n",
    "    return {\n",
    "        'model': best_lda,\n",
    "        'time': t_end - t_start,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def save_models_lda():\n",
    "    r_time = []\n",
    "    for v in (tds := tqdm(ds.columns)):\n",
    "        tds.set_description(f'Training {v}')\n",
    "        if 'B' in v: \n",
    "            r_time.append(0)\n",
    "            continue\n",
    "        model = lda(ds[v])\n",
    "        model['model'].save(f'{PATH_PREFIX_MODEL_LDA}{v}')\n",
    "        r_time.append(model['time'])\n",
    "    elapse_time['lda_training'] = pd.Series(r_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DSWP: 100%|██████████| 4/4 [00:24<00:00,  6.22s/it]\n"
     ]
    }
   ],
   "source": [
    "save_models_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>tokenizing</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>lda_training</th>\n",
       "      <th>bertopic_training</th>\n",
       "      <th>lda_evaluation</th>\n",
       "      <th>bertopic_evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dSG</td>\n",
       "      <td>4.591405</td>\n",
       "      <td>1.534670</td>\n",
       "      <td>4.760596</td>\n",
       "      <td>7.329921</td>\n",
       "      <td>1.358605</td>\n",
       "      <td>2.299849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DSG</td>\n",
       "      <td>4.591405</td>\n",
       "      <td>1.534672</td>\n",
       "      <td>4.620546</td>\n",
       "      <td>2.738254</td>\n",
       "      <td>1.660388</td>\n",
       "      <td>1.268438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dSWP</td>\n",
       "      <td>4.591405</td>\n",
       "      <td>0.573513</td>\n",
       "      <td>7.186311</td>\n",
       "      <td>7.040041</td>\n",
       "      <td>2.037808</td>\n",
       "      <td>2.152552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DSWP</td>\n",
       "      <td>4.591405</td>\n",
       "      <td>0.702334</td>\n",
       "      <td>7.947597</td>\n",
       "      <td>2.360943</td>\n",
       "      <td>1.625919</td>\n",
       "      <td>0.247443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  variant  tokenizing  preprocessing  lda_training  bertopic_training  \\\n",
       "0     dSG    4.591405       1.534670      4.760596           7.329921   \n",
       "1     DSG    4.591405       1.534672      4.620546           2.738254   \n",
       "2    dSWP    4.591405       0.573513      7.186311           7.040041   \n",
       "3    DSWP    4.591405       0.702334      7.947597           2.360943   \n",
       "\n",
       "   lda_evaluation  bertopic_evaluation  \n",
       "0        1.358605             2.299849  \n",
       "1        1.660388             1.268438  \n",
       "2        2.037808             2.152552  \n",
       "3        1.625919             0.247443  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapse_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "if USE_GPU:\n",
    "    from cuml.cluster import HDBSCAN\n",
    "    from cuml.manifold import UMAP\n",
    "\n",
    "\n",
    "def save_models_bertopic():\n",
    "    r_time = []\n",
    "    for v in (tds := tqdm(ds.columns)):\n",
    "        tds.set_description(f'Training {v}')\n",
    "        if 'T' in v: \n",
    "            r_time.append(0)\n",
    "            continue\n",
    "        t_start = time()\n",
    "\n",
    "        docs = ds[v].dropna()\n",
    "        docs = [' '.join(doc) for doc in docs] if 'B' not in v else docs\n",
    "\n",
    "        if USE_GPU:\n",
    "            umap_model = UMAP(n_components=5, n_neighbors=15, min_dist=0.0)\n",
    "            hdbscan_model = HDBSCAN(min_samples=10, gen_min_span_tree=True, prediction_data=True)\n",
    "            if 'G' in v:\n",
    "                bertopic = BERTopic(\n",
    "                    language='multilingual',\n",
    "                    umap_model=umap_model,\n",
    "                    hdbscan_model=hdbscan_model,\n",
    "                    n_gram_range=(1,3)\n",
    "                )\n",
    "            else:\n",
    "                bertopic = BERTopic(\n",
    "                    language='multilingual',\n",
    "                    umap_model=umap_model,\n",
    "                    hdbscan_model=hdbscan_model,\n",
    "                )\n",
    "        else:\n",
    "            pipe = make_pipeline(\n",
    "                TfidfVectorizer(),\n",
    "                TruncatedSVD(100)\n",
    "            )\n",
    "            if 'G' in v:\n",
    "                bertopic = BERTopic(embedding_model=pipe, n_gram_range=(1,3))\n",
    "            else:\n",
    "                bertopic = BERTopic(embedding_model=pipe)\n",
    "        model = bertopic.fit(docs)\n",
    "\n",
    "        t_end = time()\n",
    "        model.save(\n",
    "            f\"{PATH_PREFIX_MODEL_BERTOPIC}{v}\",\n",
    "            serialization=\"safetensors\",\n",
    "            save_ctfidf=True,\n",
    "        )\n",
    "        r_time.append(t_end - t_start)\n",
    "    elapse_time['bertopic_training'] = pd.Series(r_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training dSG:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DSWP: 100%|██████████| 4/4 [00:25<00:00,  6.30s/it]\n"
     ]
    }
   ],
   "source": [
    "save_models_bertopic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>tokenizing</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>lda_training</th>\n",
       "      <th>bertopic_training</th>\n",
       "      <th>lda_evaluation</th>\n",
       "      <th>bertopic_evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dSG</td>\n",
       "      <td>4.591405</td>\n",
       "      <td>1.534670</td>\n",
       "      <td>4.760596</td>\n",
       "      <td>12.345006</td>\n",
       "      <td>1.358605</td>\n",
       "      <td>2.299849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DSG</td>\n",
       "      <td>4.591405</td>\n",
       "      <td>1.534672</td>\n",
       "      <td>4.620546</td>\n",
       "      <td>3.080706</td>\n",
       "      <td>1.660388</td>\n",
       "      <td>1.268438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dSWP</td>\n",
       "      <td>4.591405</td>\n",
       "      <td>0.573513</td>\n",
       "      <td>7.186311</td>\n",
       "      <td>6.884520</td>\n",
       "      <td>2.037808</td>\n",
       "      <td>2.152552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DSWP</td>\n",
       "      <td>4.591405</td>\n",
       "      <td>0.702334</td>\n",
       "      <td>7.947597</td>\n",
       "      <td>2.630113</td>\n",
       "      <td>1.625919</td>\n",
       "      <td>0.247443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  variant  tokenizing  preprocessing  lda_training  bertopic_training  \\\n",
       "0     dSG    4.591405       1.534670      4.760596          12.345006   \n",
       "1     DSG    4.591405       1.534672      4.620546           3.080706   \n",
       "2    dSWP    4.591405       0.573513      7.186311           6.884520   \n",
       "3    DSWP    4.591405       0.702334      7.947597           2.630113   \n",
       "\n",
       "   lda_evaluation  bertopic_evaluation  \n",
       "0        1.358605             2.299849  \n",
       "1        1.660388             1.268438  \n",
       "2        2.037808             2.152552  \n",
       "3        1.625919             0.247443  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapse_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapse_time.to_csv(PATH_FILE_ELAPSE_TIME, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
