{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_FILE_PREPROCESSED = './results/ds.parquet'\n",
    "PATH_FILE_ELAPSE_TIME = './results/elapse_time.csv'\n",
    "PATH_PREFIX_MODEL_LDA = './results/models_lda/'\n",
    "PATH_PREFIX_MODEL_BERTOPIC = './results/models_bertopic/'\n",
    "LDA_N_TRIAL = 2\n",
    "USE_GPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds = pd.read_parquet(PATH_FILE_PREPROCESSED)\n",
    "elapse_time = pd.read_csv(PATH_FILE_ELAPSE_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/civbag/mambaforge/envs/lda-bertopic/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/civbag/mambaforge/envs/lda-bertopic/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/civbag/mambaforge/envs/lda-bertopic/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/civbag/mambaforge/envs/lda-bertopic/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from utils import get_diversity, get_topics_lda\n",
    "import optuna\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def lda(docs):\n",
    "    docs = docs.dropna()\n",
    "    dictionary = Dictionary(docs)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    def objective(trial, get_lda=False):\n",
    "        num_topics = trial.suggest_int('num_topics', 5, 100)\n",
    "        alpha = trial.suggest_categorical('alpha_categorical', ['symmetric', 'asymmetric', 'scalar'])\n",
    "        eta = trial.suggest_categorical('eta_categorical', ['symmetric', 'auto', 'scalar'])\n",
    "        if alpha == 'scalar':\n",
    "            alpha = trial.suggest_float('alpha', 0.01, 1)\n",
    "        if eta == 'scalar':\n",
    "            eta = trial.suggest_float('eta', 0.01, 1)\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num_topics,\n",
    "            alpha=alpha,\n",
    "            eta=eta,\n",
    "        )\n",
    "        if get_lda:\n",
    "            return model\n",
    "        c = CoherenceModel(model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "        cs = c.get_coherence()\n",
    "        ts = get_diversity(get_topics_lda(model, dictionary))\n",
    "        return cs * ts\n",
    "\n",
    "    t_start = time()\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=LDA_N_TRIAL)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_lda = objective(best_trial, get_lda=True)\n",
    "    t_end = time()\n",
    "\n",
    "    return {\n",
    "        'model': best_lda,\n",
    "        'time': t_end - t_start,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def save_models_lda():\n",
    "    r_time = []\n",
    "    for v in (tds := tqdm(ds.columns)):\n",
    "        tds.set_description(f'Training {v}')\n",
    "        if 'B' in v: \n",
    "            r_time.append(0)\n",
    "            continue\n",
    "        model = lda(ds[v])\n",
    "        model['model'].save(f'{PATH_PREFIX_MODEL_LDA}{v}')\n",
    "        r_time.append(model['time'])\n",
    "    elapse_time['lda_training'] = pd.Series(r_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DLWN: 100%|██████████| 2/2 [00:23<00:00, 11.79s/it]\n"
     ]
    }
   ],
   "source": [
    "save_models_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>nlp</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>lda_training</th>\n",
       "      <th>bertopic_training</th>\n",
       "      <th>lda_evaluation</th>\n",
       "      <th>bertopic_evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dLWN</td>\n",
       "      <td>267.22045</td>\n",
       "      <td>0.790498</td>\n",
       "      <td>14.798152</td>\n",
       "      <td>285.131878</td>\n",
       "      <td>2.156977</td>\n",
       "      <td>8.765084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DLWN</td>\n",
       "      <td>267.22045</td>\n",
       "      <td>1.186545</td>\n",
       "      <td>8.474559</td>\n",
       "      <td>30.661990</td>\n",
       "      <td>3.115971</td>\n",
       "      <td>4.547993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  variant        nlp  preprocessing  lda_training  bertopic_training  \\\n",
       "0    dLWN  267.22045       0.790498     14.798152         285.131878   \n",
       "1    DLWN  267.22045       1.186545      8.474559          30.661990   \n",
       "\n",
       "   lda_evaluation  bertopic_evaluation  \n",
       "0        2.156977             8.765084  \n",
       "1        3.115971             4.547993  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapse_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "def save_models_bertopic():\n",
    "    r_time = []\n",
    "    for v in (tds := tqdm(ds.columns)):\n",
    "        tds.set_description(f'Training {v}')\n",
    "        if 'T' in v: \n",
    "            r_time.append(0)\n",
    "            continue\n",
    "        t_start = time()\n",
    "\n",
    "        docs = ds[v].dropna()\n",
    "        docs = [' '.join(doc) for doc in docs] if 'B' not in v else docs\n",
    "\n",
    "        if USE_GPU:\n",
    "            bertopic = BERTopic(language='multilingual')\n",
    "        else:\n",
    "            pipe = make_pipeline(\n",
    "                TfidfVectorizer(),\n",
    "                TruncatedSVD(100)\n",
    "            )\n",
    "            bertopic = BERTopic(embedding_model=pipe)\n",
    "        model = bertopic.fit(docs)\n",
    "\n",
    "        t_end = time()\n",
    "        model.save(\n",
    "            f\"{PATH_PREFIX_MODEL_BERTOPIC}{v}\",\n",
    "            serialization=\"safetensors\",\n",
    "            save_ctfidf=True,\n",
    "        )\n",
    "        r_time.append(t_end - t_start)\n",
    "    elapse_time['bertopic_training'] = pd.Series(r_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training dLWN:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DLWN: 100%|██████████| 2/2 [00:36<00:00, 18.34s/it]\n"
     ]
    }
   ],
   "source": [
    "save_models_bertopic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>nlp</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>lda_training</th>\n",
       "      <th>bertopic_training</th>\n",
       "      <th>lda_evaluation</th>\n",
       "      <th>bertopic_evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dLWN</td>\n",
       "      <td>267.22045</td>\n",
       "      <td>0.790498</td>\n",
       "      <td>14.798152</td>\n",
       "      <td>30.627983</td>\n",
       "      <td>2.156977</td>\n",
       "      <td>8.765084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DLWN</td>\n",
       "      <td>267.22045</td>\n",
       "      <td>1.186545</td>\n",
       "      <td>8.474559</td>\n",
       "      <td>5.997780</td>\n",
       "      <td>3.115971</td>\n",
       "      <td>4.547993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  variant        nlp  preprocessing  lda_training  bertopic_training  \\\n",
       "0    dLWN  267.22045       0.790498     14.798152          30.627983   \n",
       "1    DLWN  267.22045       1.186545      8.474559           5.997780   \n",
       "\n",
       "   lda_evaluation  bertopic_evaluation  \n",
       "0        2.156977             8.765084  \n",
       "1        3.115971             4.547993  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapse_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapse_time.to_csv(PATH_FILE_ELAPSE_TIME, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
