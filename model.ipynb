{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_FILE_PREPROCESSED = './results/ds.parquet'\n",
    "PATH_FILE_ELAPSE_TIME = './results/elapse_time.csv'\n",
    "PATH_PREFIX_MODEL_LDA = './results/models_lda/'\n",
    "PATH_PREFIX_MODEL_BERTOPIC = './results/models_bertopic/'\n",
    "LDA_N_TRIAL = 1 # FIXME: should be 100 fr fr\n",
    "USE_GPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds = pd.read_parquet(PATH_FILE_PREPROCESSED)\n",
    "elapse_time = pd.read_csv(PATH_FILE_ELAPSE_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/civbag/repo/lda-bertopic/.venv/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/civbag/repo/lda-bertopic/.venv/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/civbag/repo/lda-bertopic/.venv/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/civbag/repo/lda-bertopic/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/civbag/repo/lda-bertopic/.venv/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from utils import get_diversity, get_topics_lda\n",
    "import optuna\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def lda(docs):\n",
    "    docs = docs.dropna()\n",
    "    dictionary = Dictionary(docs)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    def objective(trial, get_lda=False):\n",
    "        num_topics = trial.suggest_int('num_topics', 5, 100)\n",
    "        alpha = trial.suggest_categorical('alpha_categorical', ['symmetric', 'asymmetric', 'scalar'])\n",
    "        eta = trial.suggest_categorical('eta_categorical', ['symmetric', 'auto', 'scalar'])\n",
    "        if alpha == 'scalar':\n",
    "            alpha = trial.suggest_float('alpha', 0.01, 1)\n",
    "        if eta == 'scalar':\n",
    "            eta = trial.suggest_float('eta', 0.01, 1)\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num_topics,\n",
    "            alpha=alpha,\n",
    "            eta=eta,\n",
    "        )\n",
    "        if get_lda:\n",
    "            return model\n",
    "        c = CoherenceModel(model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "        cs = c.get_coherence()\n",
    "        ts = get_diversity(get_topics_lda(model, dictionary))\n",
    "        return cs * ts\n",
    "\n",
    "    t_start = time()\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=LDA_N_TRIAL)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_lda = objective(best_trial, get_lda=True)\n",
    "    t_end = time()\n",
    "\n",
    "    return {\n",
    "        'model': best_lda,\n",
    "        'time': t_end - t_start,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def save_models_lda():\n",
    "    r_time = []\n",
    "    for v in (tds := tqdm(ds.columns)):\n",
    "        tds.set_description(f'Training {v}')\n",
    "        if 'B' in v: \n",
    "            r_time.append(0)\n",
    "            continue\n",
    "        model = lda(ds[v])\n",
    "        model['model'].save(f'{PATH_PREFIX_MODEL_LDA}{v}')\n",
    "        r_time.append(model['time'])\n",
    "    elapse_time['lda_training'] = pd.Series(r_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training dCSW:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DCLW: 100%|██████████| 4/4 [00:21<00:00,  5.32s/it]\n"
     ]
    }
   ],
   "source": [
    "save_models_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>nlp</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>lda_training</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dCSW</td>\n",
       "      <td>40.981253</td>\n",
       "      <td>1.658864</td>\n",
       "      <td>3.349057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DCSW</td>\n",
       "      <td>40.981253</td>\n",
       "      <td>1.658865</td>\n",
       "      <td>7.551509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dCLW</td>\n",
       "      <td>40.981253</td>\n",
       "      <td>0.075823</td>\n",
       "      <td>6.950676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DCLW</td>\n",
       "      <td>40.981253</td>\n",
       "      <td>0.075824</td>\n",
       "      <td>3.171497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  variant        nlp  preprocessing  lda_training\n",
       "0    dCSW  40.981253       1.658864      3.349057\n",
       "1    DCSW  40.981253       1.658865      7.551509\n",
       "2    dCLW  40.981253       0.075823      6.950676\n",
       "3    DCLW  40.981253       0.075824      3.171497"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapse_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "def save_models_bertopic():\n",
    "    r_time = []\n",
    "    for v in (tds := tqdm(ds.columns)):\n",
    "        tds.set_description(f'Training {v}')\n",
    "        if 'T' in v: \n",
    "            r_time.append(0)\n",
    "            continue\n",
    "        t_start = time()\n",
    "\n",
    "        docs = ds[v].dropna()\n",
    "        docs = [' '.join(doc) for doc in docs] if 'B' not in v else docs\n",
    "\n",
    "        if USE_GPU:\n",
    "            from cuml.cluster import HDBSCAN\n",
    "            from cuml.manifold import UMAP\n",
    "            umap_model = UMAP(n_components=5, n_neighbors=15, min_dist=0.0)\n",
    "            hdbscan_model = HDBSCAN(min_samples=10, gen_min_span_tree=True, prediction_data=True)\n",
    "            bertopic = BERTopic(language='multilingual', umap_model=umap_model, hdbscan_model=hdbscan_model)\n",
    "        else:\n",
    "            pipe = make_pipeline(\n",
    "                TfidfVectorizer(),\n",
    "                TruncatedSVD(100)\n",
    "            )\n",
    "            bertopic = BERTopic(embedding_model=pipe)\n",
    "        model = bertopic.fit(docs)\n",
    "\n",
    "        t_end = time()\n",
    "        model.save(\n",
    "            f\"{PATH_PREFIX_MODEL_BERTOPIC}{v}\",\n",
    "            serialization=\"safetensors\",\n",
    "            save_ctfidf=True,\n",
    "        )\n",
    "        r_time.append(t_end - t_start)\n",
    "    elapse_time['bertopic_training'] = pd.Series(r_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DCLW: 100%|██████████| 4/4 [00:18<00:00,  4.74s/it]\n"
     ]
    }
   ],
   "source": [
    "save_models_bertopic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>nlp</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>lda_training</th>\n",
       "      <th>bertopic_training</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dCSW</td>\n",
       "      <td>40.981253</td>\n",
       "      <td>1.658864</td>\n",
       "      <td>3.349057</td>\n",
       "      <td>6.923768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DCSW</td>\n",
       "      <td>40.981253</td>\n",
       "      <td>1.658865</td>\n",
       "      <td>7.551509</td>\n",
       "      <td>2.469865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dCLW</td>\n",
       "      <td>40.981253</td>\n",
       "      <td>0.075823</td>\n",
       "      <td>6.950676</td>\n",
       "      <td>6.947025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DCLW</td>\n",
       "      <td>40.981253</td>\n",
       "      <td>0.075824</td>\n",
       "      <td>3.171497</td>\n",
       "      <td>2.534088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  variant        nlp  preprocessing  lda_training  bertopic_training\n",
       "0    dCSW  40.981253       1.658864      3.349057           6.923768\n",
       "1    DCSW  40.981253       1.658865      7.551509           2.469865\n",
       "2    dCLW  40.981253       0.075823      6.950676           6.947025\n",
       "3    DCLW  40.981253       0.075824      3.171497           2.534088"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapse_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapse_time.to_csv(PATH_FILE_ELAPSE_TIME, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
