{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_FILE_PREPROCESSED = './results/ds.parquet'\n",
    "PATH_FILE_ELAPSE_TIME = './results/elapse_time.csv'\n",
    "PATH_PREFIX_MODEL_LDA = './results/models_lda/'\n",
    "PATH_PREFIX_MODEL_BERTOPIC = './results/models_bertopic/'\n",
    "LDA_N_TRIAL = 3 # FIXME: should be 100 fr fr\n",
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds = pd.read_parquet(PATH_FILE_PREPROCESSED)\n",
    "elapse_time = pd.read_csv(PATH_FILE_ELAPSE_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab_sc/mambaforge/envs/lba/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/lab_sc/mambaforge/envs/lba/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/lab_sc/mambaforge/envs/lba/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/lab_sc/mambaforge/envs/lba/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/lab_sc/mambaforge/envs/lba/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from utils import get_diversity, get_topics_lda\n",
    "import optuna\n",
    "\n",
    "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def lda(docs):\n",
    "    docs = docs.dropna()\n",
    "    dictionary = Dictionary(docs)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    def objective(trial, get_lda=False):\n",
    "        num_topics = trial.suggest_int('num_topics', 5, 100)\n",
    "        alpha = trial.suggest_categorical('alpha_categorical', ['symmetric', 'asymmetric', 'scalar'])\n",
    "        eta = trial.suggest_categorical('eta_categorical', ['symmetric', 'auto', 'scalar'])\n",
    "        if alpha == 'scalar':\n",
    "            alpha = trial.suggest_float('alpha', 0.01, 1)\n",
    "        if eta == 'scalar':\n",
    "            eta = trial.suggest_float('eta', 0.01, 1)\n",
    "        model = LdaMulticore(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num_topics,\n",
    "            alpha=alpha,\n",
    "            eta=eta,\n",
    "        )\n",
    "        if get_lda:\n",
    "            return model\n",
    "        c = CoherenceModel(model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "        cs = c.get_coherence()\n",
    "        ts = get_diversity(get_topics_lda(model, dictionary))\n",
    "        return cs * ts\n",
    "\n",
    "    t_start = time()\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=LDA_N_TRIAL)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_lda = objective(best_trial, get_lda=True)\n",
    "    t_end = time()\n",
    "\n",
    "    return {\n",
    "        'model': best_lda,\n",
    "        'time': t_end - t_start,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def save_models_lda():\n",
    "    r_time = []\n",
    "    for v in (tds := tqdm(ds.columns)):\n",
    "        tds.set_description(f'Training {v}')\n",
    "        if 'B' in v: \n",
    "            r_time.append(0)\n",
    "            continue\n",
    "        if 'G' in v:\n",
    "            r_time.append(0)\n",
    "            continue\n",
    "        model = lda(ds[v])\n",
    "        model['model'].save(f'{PATH_PREFIX_MODEL_LDA}{v}')\n",
    "        r_time.append(model['time'])\n",
    "    elapse_time['lda_training'] = pd.Series(r_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training dC:   0%|          | 0/110 [00:00<?, ?it/s][I 2023-09-05 14:03:59,973] A new study created in memory with name: no-name-cf13343d-df66-4419-a990-d55d6e976da6\n",
      "[I 2023-09-05 14:04:15,221] Trial 0 finished with value: 0.021435612105767315 and parameters: {'num_topics': 70, 'alpha_categorical': 'symmetric', 'eta_categorical': 'auto'}. Best is trial 0 with value: 0.021435612105767315.\n",
      "[I 2023-09-05 14:04:22,009] Trial 1 finished with value: 0.08314195842097628 and parameters: {'num_topics': 10, 'alpha_categorical': 'symmetric', 'eta_categorical': 'auto'}. Best is trial 1 with value: 0.08314195842097628.\n",
      "[I 2023-09-05 14:04:33,336] Trial 2 finished with value: 0.014679326428669622 and parameters: {'num_topics': 60, 'alpha_categorical': 'asymmetric', 'eta_categorical': 'scalar', 'eta': 0.3657739122398628}. Best is trial 1 with value: 0.08314195842097628.\n",
      "Training DC:   3%|▎         | 3/110 [00:40<24:11, 13.56s/it][I 2023-09-05 14:04:39,438] A new study created in memory with name: no-name-07643b5b-db54-4405-8a2a-5d5c8769ae7f\n",
      "[I 2023-09-05 14:04:55,746] Trial 0 finished with value: 0.01364359655449972 and parameters: {'num_topics': 32, 'alpha_categorical': 'scalar', 'eta_categorical': 'auto', 'alpha': 0.40325458812522286}. Best is trial 0 with value: 0.01364359655449972.\n",
      "[I 2023-09-05 14:05:01,801] Trial 1 finished with value: 0.051127400424306184 and parameters: {'num_topics': 7, 'alpha_categorical': 'asymmetric', 'eta_categorical': 'auto'}. Best is trial 1 with value: 0.051127400424306184.\n",
      "[I 2023-09-05 14:05:33,113] Trial 2 finished with value: 0.007164763286676512 and parameters: {'num_topics': 66, 'alpha_categorical': 'symmetric', 'eta_categorical': 'scalar', 'eta': 0.9099244079269916}. Best is trial 1 with value: 0.051127400424306184.\n",
      "Training dL:   4%|▎         | 4/110 [01:39<49:30, 28.02s/it][I 2023-09-05 14:05:39,183] A new study created in memory with name: no-name-d3a091ca-b128-427c-a028-dfd7b7b6d1a2\n",
      "[I 2023-09-05 14:06:03,240] Trial 0 finished with value: 0.015641258379239346 and parameters: {'num_topics': 97, 'alpha_categorical': 'symmetric', 'eta_categorical': 'auto'}. Best is trial 0 with value: 0.015641258379239346.\n",
      "[I 2023-09-05 14:06:15,842] Trial 1 finished with value: 0.03626336560683323 and parameters: {'num_topics': 52, 'alpha_categorical': 'asymmetric', 'eta_categorical': 'symmetric'}. Best is trial 1 with value: 0.03626336560683323.\n",
      "[I 2023-09-05 14:06:23,626] Trial 2 finished with value: 0.11062424635243624 and parameters: {'num_topics': 7, 'alpha_categorical': 'asymmetric', 'eta_categorical': 'auto'}. Best is trial 2 with value: 0.11062424635243624.\n",
      "Training DL:   6%|▋         | 7/110 [02:32<37:52, 22.06s/it][I 2023-09-05 14:06:31,288] A new study created in memory with name: no-name-4aaa6f32-b658-4969-9361-a2ce78a43e6c\n",
      "[I 2023-09-05 14:07:15,335] Trial 0 finished with value: 0.0060307848675766685 and parameters: {'num_topics': 79, 'alpha_categorical': 'asymmetric', 'eta_categorical': 'scalar', 'eta': 0.9368261224110892}. Best is trial 0 with value: 0.0060307848675766685.\n",
      "[I 2023-09-05 14:08:01,390] Trial 1 finished with value: 0.005769356355815218 and parameters: {'num_topics': 87, 'alpha_categorical': 'asymmetric', 'eta_categorical': 'scalar', 'eta': 0.6036828385696782}. Best is trial 0 with value: 0.0060307848675766685.\n",
      "[I 2023-09-05 14:08:51,154] Trial 2 finished with value: 0.006079846713375407 and parameters: {'num_topics': 99, 'alpha_categorical': 'asymmetric', 'eta_categorical': 'scalar', 'eta': 0.49828849711937245}. Best is trial 2 with value: 0.006079846713375407.\n"
     ]
    }
   ],
   "source": [
    "save_models_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>tokenizing</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>lda_training</th>\n",
       "      <th>bertopic_training</th>\n",
       "      <th>lda_evaluation</th>\n",
       "      <th>bertopic_evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dLG</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.076667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.829835</td>\n",
       "      <td>0.522905</td>\n",
       "      <td>5.086215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DLG</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.076667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.951725</td>\n",
       "      <td>0.600327</td>\n",
       "      <td>4.920509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dSP</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>2.339595</td>\n",
       "      <td>1.555440</td>\n",
       "      <td>5.861379</td>\n",
       "      <td>0.708510</td>\n",
       "      <td>1.559066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DSP</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>2.743002</td>\n",
       "      <td>6.910429</td>\n",
       "      <td>1.804240</td>\n",
       "      <td>0.636789</td>\n",
       "      <td>1.543857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dLN</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.080067</td>\n",
       "      <td>1.499174</td>\n",
       "      <td>5.048638</td>\n",
       "      <td>0.488897</td>\n",
       "      <td>2.605630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DLN</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.080067</td>\n",
       "      <td>9.134171</td>\n",
       "      <td>1.850412</td>\n",
       "      <td>0.789871</td>\n",
       "      <td>1.398466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dLW</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.155874</td>\n",
       "      <td>1.580441</td>\n",
       "      <td>5.598723</td>\n",
       "      <td>0.499317</td>\n",
       "      <td>1.545107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DLW</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.155875</td>\n",
       "      <td>6.300422</td>\n",
       "      <td>1.933671</td>\n",
       "      <td>0.439239</td>\n",
       "      <td>1.526137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dB</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.006221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.644242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.675596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DB</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.006221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.863791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.451739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dT</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.059530</td>\n",
       "      <td>2.474827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469568</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DT</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.059530</td>\n",
       "      <td>4.842892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.492003</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variant  tokenizing  preprocessing  lda_training  bertopic_training  \\\n",
       "0      dLG    2.061464       0.076667      0.000000           9.829835   \n",
       "1      DLG    2.061464       0.076667      0.000000           2.951725   \n",
       "2      dSP    2.061464       2.339595      1.555440           5.861379   \n",
       "3      DSP    2.061464       2.743002      6.910429           1.804240   \n",
       "4      dLN    2.061464       0.080067      1.499174           5.048638   \n",
       "5      DLN    2.061464       0.080067      9.134171           1.850412   \n",
       "6      dLW    2.061464       0.155874      1.580441           5.598723   \n",
       "7      DLW    2.061464       0.155875      6.300422           1.933671   \n",
       "8       dB    2.061464       0.006221      0.000000           5.644242   \n",
       "9       DB    2.061464       0.006221      0.000000           1.863791   \n",
       "10      dT    2.061464       0.059530      2.474827           0.000000   \n",
       "11      DT    2.061464       0.059530      4.842892           0.000000   \n",
       "\n",
       "    lda_evaluation  bertopic_evaluation  \n",
       "0         0.522905             5.086215  \n",
       "1         0.600327             4.920509  \n",
       "2         0.708510             1.559066  \n",
       "3         0.636789             1.543857  \n",
       "4         0.488897             2.605630  \n",
       "5         0.789871             1.398466  \n",
       "6         0.499317             1.545107  \n",
       "7         0.439239             1.526137  \n",
       "8         0.000000             1.675596  \n",
       "9         0.000000             1.451739  \n",
       "10        0.469568             0.000000  \n",
       "11        0.492003             0.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapse_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "if USE_GPU:\n",
    "    from cuml.cluster import HDBSCAN\n",
    "    from cuml.manifold import UMAP\n",
    "\n",
    "\n",
    "def save_models_bertopic():\n",
    "    r_time = []\n",
    "    for v in (tds := tqdm(ds.columns)):\n",
    "        tds.set_description(f'Training {v}')\n",
    "        if 'T' in v: \n",
    "            r_time.append(0)\n",
    "            continue\n",
    "        t_start = time()\n",
    "\n",
    "        docs = ds[v].dropna()\n",
    "        docs = [' '.join(doc) for doc in docs] if 'B' not in v else docs\n",
    "\n",
    "        if USE_GPU:\n",
    "            umap_model = UMAP(n_components=5, n_neighbors=15, min_dist=0.0)\n",
    "            hdbscan_model = HDBSCAN(min_samples=10, gen_min_span_tree=True, prediction_data=True)\n",
    "            if 'G' in v:\n",
    "                bertopic = BERTopic(\n",
    "                    language='multilingual',\n",
    "                    umap_model=umap_model,\n",
    "                    hdbscan_model=hdbscan_model,\n",
    "                    n_gram_range=(1,3)\n",
    "                )\n",
    "            else:\n",
    "                bertopic = BERTopic(\n",
    "                    language='multilingual',\n",
    "                    umap_model=umap_model,\n",
    "                    hdbscan_model=hdbscan_model,\n",
    "                )\n",
    "        else:\n",
    "            pipe = make_pipeline(\n",
    "                TfidfVectorizer(),\n",
    "                TruncatedSVD(100)\n",
    "            )\n",
    "            if 'G' in v:\n",
    "                bertopic = BERTopic(embedding_model=pipe, n_gram_range=(1,3))\n",
    "            else:\n",
    "                bertopic = BERTopic(embedding_model=pipe)\n",
    "        model = bertopic.fit(docs)\n",
    "\n",
    "        t_end = time()\n",
    "        model.save(\n",
    "            f\"{PATH_PREFIX_MODEL_BERTOPIC}{v}\",\n",
    "            serialization=\"safetensors\",\n",
    "            save_ctfidf=True,\n",
    "        )\n",
    "        r_time.append(t_end - t_start)\n",
    "    elapse_time['bertopic_training'] = pd.Series(r_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DT: 100%|██████████| 12/12 [00:44<00:00,  3.72s/it]\n"
     ]
    }
   ],
   "source": [
    "save_models_bertopic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>tokenizing</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>lda_training</th>\n",
       "      <th>bertopic_training</th>\n",
       "      <th>lda_evaluation</th>\n",
       "      <th>bertopic_evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dLG</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.076667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.915040</td>\n",
       "      <td>0.522905</td>\n",
       "      <td>5.086215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DLG</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.076667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.236732</td>\n",
       "      <td>0.600327</td>\n",
       "      <td>4.920509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dSP</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>2.339595</td>\n",
       "      <td>1.555440</td>\n",
       "      <td>5.950782</td>\n",
       "      <td>0.708510</td>\n",
       "      <td>1.559066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DSP</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>2.743002</td>\n",
       "      <td>6.910429</td>\n",
       "      <td>1.896040</td>\n",
       "      <td>0.636789</td>\n",
       "      <td>1.543857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dLN</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.080067</td>\n",
       "      <td>1.499174</td>\n",
       "      <td>5.109100</td>\n",
       "      <td>0.488897</td>\n",
       "      <td>2.605630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DLN</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.080067</td>\n",
       "      <td>9.134171</td>\n",
       "      <td>1.716769</td>\n",
       "      <td>0.789871</td>\n",
       "      <td>1.398466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dLW</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.155874</td>\n",
       "      <td>1.580441</td>\n",
       "      <td>5.535383</td>\n",
       "      <td>0.499317</td>\n",
       "      <td>1.545107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DLW</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.155875</td>\n",
       "      <td>6.300422</td>\n",
       "      <td>1.939281</td>\n",
       "      <td>0.439239</td>\n",
       "      <td>1.526137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dB</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.006221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.962377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.675596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DB</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.006221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.911780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.451739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dT</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.059530</td>\n",
       "      <td>2.474827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469568</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DT</td>\n",
       "      <td>2.061464</td>\n",
       "      <td>0.059530</td>\n",
       "      <td>4.842892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.492003</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variant  tokenizing  preprocessing  lda_training  bertopic_training  \\\n",
       "0      dLG    2.061464       0.076667      0.000000          10.915040   \n",
       "1      DLG    2.061464       0.076667      0.000000           3.236732   \n",
       "2      dSP    2.061464       2.339595      1.555440           5.950782   \n",
       "3      DSP    2.061464       2.743002      6.910429           1.896040   \n",
       "4      dLN    2.061464       0.080067      1.499174           5.109100   \n",
       "5      DLN    2.061464       0.080067      9.134171           1.716769   \n",
       "6      dLW    2.061464       0.155874      1.580441           5.535383   \n",
       "7      DLW    2.061464       0.155875      6.300422           1.939281   \n",
       "8       dB    2.061464       0.006221      0.000000           5.962377   \n",
       "9       DB    2.061464       0.006221      0.000000           1.911780   \n",
       "10      dT    2.061464       0.059530      2.474827           0.000000   \n",
       "11      DT    2.061464       0.059530      4.842892           0.000000   \n",
       "\n",
       "    lda_evaluation  bertopic_evaluation  \n",
       "0         0.522905             5.086215  \n",
       "1         0.600327             4.920509  \n",
       "2         0.708510             1.559066  \n",
       "3         0.636789             1.543857  \n",
       "4         0.488897             2.605630  \n",
       "5         0.789871             1.398466  \n",
       "6         0.499317             1.545107  \n",
       "7         0.439239             1.526137  \n",
       "8         0.000000             1.675596  \n",
       "9         0.000000             1.451739  \n",
       "10        0.469568             0.000000  \n",
       "11        0.492003             0.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapse_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapse_time.to_csv(PATH_FILE_ELAPSE_TIME, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
